{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"log.txt\", \"w\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"lr_search_space_log.txt\", \"a\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/kaggle/input/fineweb-10b-edu/fineweb10B-edu\"\n",
    "vocab_size = 50304 \n",
    "block_size = 512\n",
    "total_batch_size = 20480\n",
    "batch_size = 4\n",
    "assert total_batch_size % (batch_size * block_size) == 0, \"total_batch_size must be divisble by B * T\"\n",
    "grad_accum_steps = total_batch_size // (batch_size * block_size)\n",
    "weight_decay = 0.1\n",
    "epochs = 500 \n",
    "lr = 3e-4\n",
    "max_steps = epochs\n",
    "warmup_steps = 50\n",
    "max_lr = lr\n",
    "min_lr = max_lr * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using device: {device}\")\n",
    "print(grad_accum_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32)\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        assert split in {'train', 'val'}\n",
    "\n",
    "        data_root = dataset_path\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
    "        print(f\"found {len(shards)} shards for split {split}\")\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # state, init at shard zero\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        self.n_embd = n_embed\n",
    "        self.n_head = n_head\n",
    "        super().__init__()\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(self.n_embd, 3 * self.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 *  embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim), \n",
    "        )\n",
    "        self.scale_init = 1\n",
    "    def forward(self, x):\n",
    "        return self.MLP(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout):\n",
    "        super(Block, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.LayerNorm1 = nn.LayerNorm(embed_dim) \n",
    "        self.MultiheadAttention = CausalSelfAttention(self.embed_dim, self.num_heads)\n",
    "        self.LayerNorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.MLP = MLP(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.MultiheadAttention(self.LayerNorm1(x))\n",
    "        x = x + self.MLP(self.LayerNorm2(x))\n",
    "        return x\n",
    "\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self, block_size, vocab_size, embed_dim, num_heads, num_blocks, dropout):\n",
    "        super(Model1, self).__init__()\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim  \n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.positional_embedding = nn.Embedding(self.block_size, self.embed_dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList([Block(self.embed_dim, self.num_heads, self.dropout) for _ in range(self.num_blocks)])\n",
    "\n",
    "        self.LayerNorm3 = nn.LayerNorm(self.embed_dim)\n",
    "        self.lm_linear = nn.Linear(self.embed_dim, self.vocab_size)\n",
    "        \n",
    "        #weight sharing scheme\n",
    "        self.lm_linear.weight = self.embedding.weight\n",
    "\n",
    "        # Apply parameter initialization as per GPT2 model\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = 0.02\n",
    "        if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
    "            std *= (2 * self.num_blocks) ** -0.5\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.positional_embedding(torch.arange(T, device=device))\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        output = self.LayerNorm3(x)\n",
    "        output = self.lm_linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoaderLite(B=batch_size, T=block_size, split=\"train\")\n",
    "val_dataloader = DataLoaderLite(B=batch_size, T=block_size, split=\"val\")\n",
    "\n",
    "fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "use_fused = fused_available and device == \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_search_space = [1e-2, 9e-3, 5e-3, 3e-3, 2e-3, 1e-3, 9e-4, 6e-4, 5e-4, 3e-4, 1e-4, 9e-5, 6e-5, 3e-5, 9e-6, 5e-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr in lr_search_space:\n",
    "    model = Model1(block_size=block_size, vocab_size=vocab_size, embed_dim=512, num_heads=16, num_blocks=8, dropout=0.2).to(device)\n",
    "    param_dict = [p for p in model.parameters()]\n",
    "    param_dict = [p for p in param_dict if p.requires_grad]\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "    decay_params = [p for p in param_dict if p.dim() >= 2]\n",
    "    nodecay_params = [p for p in param_dict if p.dim() < 2]\n",
    "    optim_groups = [\n",
    "        {'params': decay_params, 'weight_decay': weight_decay},\n",
    "        {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(optim_groups, lr=lr, fused=use_fused)\n",
    "    initial_lr = lr\n",
    "    final_train_loss = 0.0\n",
    "    final_val_loss = 0.0\n",
    "    Norm_final = 0.0\n",
    "    for epoch in range(epochs): #epochs\n",
    "        optimizer.zero_grad()\n",
    "        loss_acum = 0.0\n",
    "        for micro_step in range(grad_accum_steps): #grad_accum_steps\n",
    "            x, y = train_dataloader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            train_loss = loss(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            \n",
    "            train_loss = train_loss / grad_accum_steps\n",
    "            loss_acum += train_loss.detach()\n",
    "            train_loss.backward()\n",
    "            final_train_loss = loss_acum.item()\n",
    "            \n",
    "\n",
    "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "        lr = get_lr(epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x, y = val_dataloader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            val_loss = loss(output.view(-1, output.size(-1)), y.view(-1))\n",
    "            final_val_loss = val_loss\n",
    "        \n",
    "        Norm_final = norm\n",
    "        print_statement = f\"Epoch: {epoch}| Train Loss: {loss_acum.item()} | Val Loss: {val_loss.item()} | Norm: {norm} | lr: {lr}\"\n",
    "        with open(\"log.txt\", \"a\") as file:\n",
    "            file.write(f\"{print_statement}\\n\")\n",
    "        print(print_statement)\n",
    "        \n",
    "    print(f\"For learning rate: {initial_lr} | Train Loss: {final_train_loss} | Val Loss: {final_val_loss} | Norm: {Norm_final}\")\n",
    "    with open(\"lr_search_space_log.txt\", \"a\") as file:\n",
    "        file.write(f\"For learning rate {initial_lr} | Train Loss: {final_train_loss} | Val Loss: {final_val_loss} | Norm: {Norm_final}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-pytroch-compoutervision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
